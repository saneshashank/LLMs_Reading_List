# LLMs_Reading_List
List of useful articles/papers/videos to read watch

### papers
#### RoPE paper: https://arxiv.org/pdf/2104.09864
#### YaRN (Yet another RoPE extensioN method) Efficient Context Window Extension of Large Language Models: https://arxiv.org/html/2309.00071v3
#### SmolLM2: When Smol Goes Big â€” Data-Centric Training of a Small Language Model: https://arxiv.org/pdf/2502.02737
#### Lessons from Scaling Synthetic Data- for Trillion-scale Pretraining: https://arxiv.org/pdf/2508.10975
#### Influential Subset Selection for Language Modelhttps://arxiv.org/pdf/2305.12816

#### Muon Optimizer: https://huggingface.co/blog/onekq/muon-optimizer

#### Rotary Positional Embeddings: Combining Absolute and Relative https://www.youtube.com/watch?v=o29P0Kpobz0&t=530s

#### Quantization Aware Training (QAT): Pretraining Large Language Models with NVFP4 (NVIDIA) - https://arxiv.org/pdf/2509.25149

#### Scaling Latent Reasoning via Looped Language Models: https://arxiv.org/html/2510.25741v2#S1

### Qwen3-Next: Towards Ultimate Training & Inference Efficiency: https://qwen.ai/blog?id=4074cca80393150c248e508aa62983f9cb7d27cd&from=research.latest-advancements-list&ref=aiforradalom.poltextlab.com

#### null MOE: https://arxiv.org/html/2601.15370v1
#### Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models:https://arxiv.org/html/2601.07372v1

### mHC: Manifold-Constrained Hyper-Connections: https://arxiv.org/html/2512.24880v2

### Model Growth
* FLM-101B: An Open LLM and How to Train It with $100K Budget:https://arxiv.org/html/2309.03852v3
* Qwen3-Next: Towards Ultimate Training & Inference Efficiency: https://qwen.ai/blog?id=4074cca80393150c248e508aa62983f9cb7d27cd&from=research.latest-advancements-list&ref=aiforradalom.poltextlab.com

#### The Depth Delusion: https://arxiv.org/html/2601.20994v1

### Efficient kernels
* https://github.com/linkedin/Liger-Kernel

* https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1
* https://huggingface.co/spaces/nanotron/ultrascale-playbook?section=high-level_overview

#### Reversing Large Language Models for Efficient Training and Fine-Tuning: https://arxiv.org/html/2512.02056v1

### DeepSpeed: https://www.deepspeed.ai/tutorials/mixture-of-experts-nlg/

### From Single GPU to Clusters: A Practical Journey into Distributed Training with PyTorch and Ray: https://debnsuma.github.io/my-blog/posts/distributed-training-from-scratch/

### Inside FSDP with PyTorch and Ray: Scaling Model Training with Fully Sharded Data Parallel: https://debnsuma.github.io/my-blog/posts/fsdp-ray-train/#deepspeed-an-alternative-to-fsdp2


