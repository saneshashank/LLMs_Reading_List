# LLMs_Reading_List
List of useful articles/papers/videos to read watch

### papers
#### RoPE paper: https://arxiv.org/pdf/2104.09864
#### SmolLM2: When Smol Goes Big â€” Data-Centric Training of a Small Language Model: https://arxiv.org/pdf/2502.02737
#### Lessons from Scaling Synthetic Data- for Trillion-scale Pretraining: https://arxiv.org/pdf/2508.10975
#### Influential Subset Selection for Language Modelhttps://arxiv.org/pdf/2305.12816

#### Muon Optimizer: https://huggingface.co/blog/onekq/muon-optimizer

#### Rotary Positional Embeddings: Combining Absolute and Relative https://www.youtube.com/watch?v=o29P0Kpobz0&t=530s

#### Quantization Aware Training (QAT): Pretraining Large Language Models with NVFP4 (NVIDIA) - https://arxiv.org/pdf/2509.25149

#### Scaling Latent Reasoning via Looped Language Models: https://arxiv.org/html/2510.25741v2#S1

### code/repos
#### llama3-from-scratch: https://github.com/naklecha/llama3-from-scratch/blob/main/llama3-from-scratch.ipynb

#### null MOE: https://arxiv.org/html/2601.15370v1
#### Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models:https://arxiv.org/html/2601.07372v1

### Model Growth
* FLM-101B: An Open LLM and How to Train It with $100K Budget:https://arxiv.org/html/2309.03852v3
* Qwen3-Next: Towards Ultimate Training & Inference Efficiency: https://qwen.ai/blog?id=4074cca80393150c248e508aa62983f9cb7d27cd&from=research.latest-advancements-list&ref=aiforradalom.poltextlab.com



